{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A provisional restaraunt rating system that estimates the success of an establishment based on it's location, type and cuisine among other factors. The model is a neural network that runs on a Keras framework, trained on the 'Zomato Bangalore Restaurants' dataset curated by Himanshu Poddar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd \n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = 50000 # size of dataset\n",
    "df1 = pd.read_csv('zomato.csv', delimiter=',', nrows = nRowsRead)\n",
    "df1.dataframeName = 'zomato.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded and transformed into the required format before feeding the model. Location and cuisine were one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4.1/5' 775 'Banashankari' 'Casual Dining'\n",
      "  'North Indian, Mughlai, Chinese' '800']\n",
      " ['4.1/5' 787 'Banashankari' 'Casual Dining'\n",
      "  'Chinese, North Indian, Thai' '800']\n",
      " ['3.8/5' 918 'Banashankari' 'Cafe, Casual Dining'\n",
      "  'Cafe, Mexican, Italian' '800']\n",
      " ...\n",
      " ['3.6 /5' 19 'Bellandur' 'Food Court' 'Continental, Pizza' '500']\n",
      " ['3.0 /5' 155 'Bellandur' 'Casual Dining' 'North Indian, Biryani' '800']\n",
      " ['3.0 /5' 13 'Bellandur' 'Quick Bites' 'North Indian, Chinese' '400']]\n",
      "{'Banashankari': 0, 'Basavanagudi': 1, 'Mysore Road': 2, 'Jayanagar': 3, 'Kumaraswamy Layout': 4, 'Rajarajeshwari Nagar': 5, 'Vijay Nagar': 6, 'Uttarahalli': 7, 'JP Nagar': 8, 'South Bangalore': 9, 'City Market': 10, 'Nagarbhavi': 11, 'Bannerghatta Road': 12, 'BTM': 13, 'Kanakapura Road': 14, 'Bommanahalli': 15, 'CV Raman Nagar': 16, 'Electronic City': 17, 'HSR': 18, 'Marathahalli': 19, 'Sarjapur Road': 20, 'Wilson Garden': 21, 'Shanti Nagar': 22, 'Koramangala 5th Block': 23, 'Koramangala 8th Block': 24, 'Richmond Road': 25, 'Koramangala 7th Block': 26, 'Jalahalli': 27, 'Koramangala 4th Block': 28, 'Bellandur': 29, 'Whitefield': 30, 'East Bangalore': 31, 'Old Airport Road': 32, 'Indiranagar': 33, 'Koramangala 1st Block': 34, 'Frazer Town': 35, 'RT Nagar': 36, 'MG Road': 37, 'Brigade Road': 38, 'Lavelle Road': 39, 'Church Street': 40, 'Ulsoor': 41, 'Residency Road': 42, 'Shivajinagar': 43, 'Infantry Road': 44, 'St. Marks Road': 45, 'Cunningham Road': 46, 'Race Course Road': 47, 'Commercial Street': 48, 'Vasanth Nagar': 49, 'HBR Layout': 50, 'Domlur': 51, 'Ejipura': 52, 'Jeevan Bhima Nagar': 53, 'Old Madras Road': 54, 'Malleshwaram': 55, 'Seshadripuram': 56, 'Kammanahalli': 57, 'Koramangala 6th Block': 58, 'Majestic': 59, 'Langford Town': 60, 'Central Bangalore': 61, 'Sanjay Nagar': 62, 'Brookefield': 63, 'ITPL Main Road, Whitefield': 64, 'Varthur Main Road, Whitefield': 65, 'KR Puram': 66, 'Koramangala 2nd Block': 67, 'Koramangala 3rd Block': 68, 'Koramangala': 69, 'Hosur Road': 70, 'Rajajinagar': 71, 'Banaswadi': 72, 'North Bangalore': 73, 'Nagawara': 74, 'Hennur': 75, 'Kalyan Nagar': 76, 'New BEL Road': 77, 'Jakkur': 78, 'Rammurthy Nagar': 79, 'Thippasandra': 80, 'Kaggadasapura': 81, 'Hebbal': 82, 'Kengeri': 83, 'Sankey Road': 84, 'Sadashiv Nagar': 85, 'Basaveshwara Nagar': 86, 'Yeshwantpur': 87, 'West Bangalore': 88, 'Magadi Road': 89, 'Yelahanka': 90, 'Sahakara Nagar': 91, 'Peenya': 92}\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "D = np.array(df1.values)\n",
    "D = D[:,[5,6,8,9,11,12]]\n",
    "print(D)\n",
    "\n",
    "#One-hot encoded location\n",
    "LDict = {}\n",
    "cnt = 0\n",
    "for i in range(len(D[:,2])):\n",
    "    if(type(D[i,2])==str):\n",
    "        x = D[i,2]\n",
    "        if x not in LDict:\n",
    "            LDict.update({x : cnt})\n",
    "            cnt = cnt + 1\n",
    "print(LDict)\n",
    "for i in range(len(D[:,2])):\n",
    "    if (type(D[i,2]) == str):\n",
    "        D[i,2] = LDict[D[i,2]]\n",
    "    else:\n",
    "        D[i,2] = random.randint(0,len(LDict)-1)\n",
    "L_onehot  = to_categorical(D[:,2])\n",
    "print(L_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Casual Dining': 0, 'Cafe': 1, 'Quick Bites': 2, 'Delivery': 3, 'Mess': 4, 'Dessert Parlor': 5, 'Bakery': 6, 'Pub': 7, 'Takeaway': 8, 'Fine Dining': 9, 'Beverage Shop': 10, 'Sweet Shop': 11, 'Bar': 12, 'Confectionery': 13, 'Kiosk': 14, 'Food Truck': 15, 'Microbrewery': 16, 'Lounge': 17, 'Food Court': 18, 'Dhaba': 19, 'Club': 20, 'Irani Cafee': 21, 'Bhojanalya': 22, 'Pop Up': 23, 'Meat Shop': 24}\n"
     ]
    }
   ],
   "source": [
    "#Restaurant type dictionary\n",
    "TDict = {}\n",
    "cnt = 0\n",
    "for i in range(len(D[:,3])):\n",
    "    if(type(D[i,3])==str):\n",
    "        x = D[i,3].split(',')\n",
    "        #print(x)\n",
    "        x = np.asarray(x)\n",
    "        #print(x)\n",
    "        for j in range(len(x)):\n",
    "            x[j] = x[j].strip()\n",
    "            if x[j] not in TDict:\n",
    "                TDict.update({x[j] : cnt})\n",
    "                cnt = cnt+1\n",
    "print(TDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[800.]\n",
      " [800.]\n",
      " [800.]\n",
      " ...\n",
      " [500.]\n",
      " [800.]\n",
      " [400.]]\n"
     ]
    }
   ],
   "source": [
    "#Cost per person\n",
    "Cost = np.zeros((len(D[:,5]),1))\n",
    "for i in range(len(D[:,5])):\n",
    "    if(type(D[i,5]) == str):\n",
    "        #print(D[i,5])\n",
    "        D[i,5] = D[i,5].replace(',','')\n",
    "        if(D[i,5] != 'B'):\n",
    "            Cost[i] = int(D[i,5])\n",
    "        else:\n",
    "            Cost[i] = 0\n",
    "print(Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings\n",
    "S = np.zeros((len(D[:,0]),1))\n",
    "for i in range(len(D[:,0])):\n",
    "    if(type(D[i,0]) == str):\n",
    "        x = D[i,0].split('/')\n",
    "        x = np.asarray(x)\n",
    "        #print(x)\n",
    "        if(x[0] != 'NEW' and x[0] != '-'):\n",
    "            S[i] = float(x[0])\n",
    "        else:\n",
    "            S[i] = 0\n",
    "#print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'North Indian': 0, 'Mughlai': 1, 'Chinese': 2, 'Thai': 3, 'Cafe': 4, 'Mexican': 5, 'Italian': 6, 'South Indian': 7, 'Rajasthani': 8, 'Andhra': 9, 'Pizza': 10, 'Continental': 11, 'Momos': 12, 'Beverages': 13, 'Fast Food': 14, 'American': 15, 'French': 16, 'European': 17, 'Bakery': 18, 'Burger': 19, 'Desserts': 20, 'Biryani': 21, 'Street Food': 22, 'Rolls': 23, 'Ice Cream': 24, 'Healthy Food': 25, 'Salad': 26, 'Asian': 27, 'Korean': 28, 'Indonesian': 29, 'Japanese': 30, 'Goan': 31, 'Seafood': 32, 'Kebab': 33, 'Steak': 34, 'Mithai': 35, 'Iranian': 36, 'Sandwich': 37, 'Juices': 38, 'Mangalorean': 39, 'Vietnamese': 40, 'Hyderabadi': 41, 'Bengali': 42, 'Arabian': 43, 'BBQ': 44, 'Tea': 45, 'Afghani': 46, 'Lebanese': 47, 'Finger Food': 48, 'Tibetan': 49, 'Charcoal Chicken': 50, 'Middle Eastern': 51, 'Mediterranean': 52, 'Wraps': 53, 'Kerala': 54, 'Oriya': 55, 'Bihari': 56, 'Roast Chicken': 57, 'Maharashtrian': 58, 'Bohri': 59, 'African': 60, 'Nepalese': 61, 'Turkish': 62, 'Tamil': 63, 'Tex-Mex': 64, 'Belgian': 65, 'Gujarati': 66, 'South American': 67, 'Konkan': 68, 'Drinks Only': 69, 'Awadhi': 70, 'Chettinad': 71, 'Coffee': 72, 'Indian': 73, 'Afghan': 74, 'Modern Indian': 75, 'Lucknowi': 76, 'Australian': 77, 'Kashmiri': 78, 'Spanish': 79, 'Malaysian': 80, 'Burmese': 81, 'Sushi': 82, 'Portuguese': 83, 'Parsi': 84, 'Greek': 85, 'North Eastern': 86, 'Bar Food': 87, 'Singaporean': 88, 'Naga': 89, 'Cantonese': 90, 'Grill': 91, 'Bubble Tea': 92, 'Hot dogs': 93, 'Assamese': 94, 'Sri Lankan': 95, 'Mongolian': 96, 'Paan': 97, 'British': 98, 'Pan Asian': 99, 'German': 100, 'Russian': 101, 'Jewish': 102, 'Vegan': 103, 'Raw Meats': 104, 'Malwani': 105, 'Sindhi': 106}\n"
     ]
    }
   ],
   "source": [
    "#Cuisine dictionary\n",
    "CDict = {}\n",
    "cnt = 0\n",
    "for i in range(len(D[:,4])):\n",
    "    if(type(D[i,4])==str):\n",
    "        x = D[i,4].split(',')\n",
    "        #print(x)\n",
    "        x = np.asarray(x)\n",
    "        #print(x)\n",
    "        for j in range(len(x)):\n",
    "            x[j] = x[j].strip()\n",
    "            if x[j] not in CDict:\n",
    "                CDict.update({x[j] : cnt})\n",
    "                cnt = cnt+1\n",
    "print(CDict)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "[[4.1]\n",
      " [4.1]\n",
      " [3.8]\n",
      " ...\n",
      " [3.6]\n",
      " [3. ]\n",
      " [3. ]]\n"
     ]
    }
   ],
   "source": [
    "#Final Data Entry\n",
    "print(len(D))\n",
    "X = np.zeros((len(D),len(LDict) + len(TDict) + len(CDict) + 2))\n",
    "for i in range(len(X)):\n",
    "    y = np.zeros(len(CDict))\n",
    "    t = np.zeros(len(TDict))\n",
    "    if(type(D[i,4])==str):\n",
    "        x = D[i,4].split(',')\n",
    "        #print(x)\n",
    "        x = np.asarray(x)\n",
    "        #print(x)\n",
    "        cnt = 0\n",
    "        for j in range(len(x)):\n",
    "            x[j] = x[j].strip()\n",
    "            val = CDict[x[j]]\n",
    "            y[val] = 1\n",
    "            cnt = cnt+1\n",
    "        #Checking if every restaurant has a valid cuisine\n",
    "        if(np.sum(y)==0):\n",
    "            print(\"Indefinite cuisine\")\n",
    "    if(type(D[i,3])==str):\n",
    "        x1 = D[i,3].split(',')\n",
    "        #print(x)\n",
    "        x1 = np.asarray(x1)\n",
    "        #print(x)\n",
    "        cnt = 0\n",
    "        for j in range(len(x1)):\n",
    "            x1[j] = x1[j].strip()\n",
    "            val = TDict[x1[j]]\n",
    "            t[val] = 1\n",
    "            cnt = cnt+1    \n",
    "    #print(len(L_onehot[i]))\n",
    "    X[i,:] = np.r_[L_onehot[i],t,y,D[i,1],Cost[i]]\n",
    "#print(X)\n",
    "\n",
    "#Cuisine scores\n",
    "Y  = np.zeros((len(D),1))\n",
    "for i in range(len(Y)):\n",
    "    Y[i] = S[i]\n",
    "        \n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data now ready for training and testing. Split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n"
     ]
    }
   ],
   "source": [
    "#Preparing training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network consists of 2 hidden layers, using Relu activation. The output layer uses a linear activation. Dropout is also included to allow the model to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 11:41:21.824674 140193057347328 deprecation.py:506] From /home/naveen/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 100)               22800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 33,921\n",
      "Trainable params: 33,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Constructing NN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation = \"relu\", input_shape=(X_train.shape[1], )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "#model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(10, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(y_train.shape[1], activation = \"linear\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model runs using an Adam optimizer, with a mean squared loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling model\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 4s 100us/step - loss: 8.3050 - val_loss: 3.7723\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 3s 60us/step - loss: 3.5236 - val_loss: 2.8420\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 3s 68us/step - loss: 2.7959 - val_loss: 2.4229\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 3s 62us/step - loss: 2.0355 - val_loss: 1.4601\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 3s 58us/step - loss: 1.3602 - val_loss: 0.9013\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 3s 63us/step - loss: 0.9922 - val_loss: 0.7806\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 3s 63us/step - loss: 0.8524 - val_loss: 0.6796\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 3s 62us/step - loss: 0.7257 - val_loss: 0.6423\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 3s 67us/step - loss: 0.6509 - val_loss: 0.4341\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 3s 62us/step - loss: 0.5647 - val_loss: 0.3376\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "results = model.fit(\n",
    " X_train, y_train,\n",
    " epochs= 10,\n",
    " batch_size = 100,\n",
    " validation_data = (X_test,y_test )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss is quite satisfactory, and is an indication of the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
